@article{Hopfield1982,
  author = {Hopfield, John J.},
  title = {Neural networks and physical systems with emergent collective computational abilities.},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {79},
  number = {8},
  pages = {2554-2558},
  year = {1982},
  doi = {10.1073/pnas.79.8.2554},
  URL = {https://www.pnas.org/doi/abs/10.1073/pnas.79.8.2554},
  eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.79.8.2554},
  keywords = {neural network type:recurrent, evaluation type:mathematical proof},
  series = {PNAS},
  abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.}
}
@article{HopfieldTank1986,
  author = {Hopfield, John J. and Tank, David W.},
  title = {Computing with Neural Circuits: A Model},
  journal = {Science},
  volume = {233},
  number = {4764},
  pages = {625-633},
  year = {1986},
  doi = {10.1126/science.3755256},
  URL = {https://www.science.org/doi/abs/10.1126/science.3755256},
  eprint = {https://www.science.org/doi/pdf/10.1126/science.3755256},
  series = {Science},
  keywords = {neural network type:recurrent, evaluation type:mathematical proof, evaluation type:qualitative assessment},
  abstract = {A new conceptual framework and a minimization principle together provide an understanding of computation in model neural circuits. The circuits consist of nonlinear graded-response model neurons organized into networks with effectively symmetric synaptic connections. The neurons represent an approximation to biological neurons in which a simplified set of important computational properties is retained. Complex circuits solving problems similar to those essential in biology can be analyzed and understood without the need to follow the circuit dynamics in detail. Implementation of the model with electronic devices will provide a class of electronic circuits of novel form and function.}
}
@article{Werbos1988,
  title = {Generalization of Backpropagation with Application to a Recurrent Gas Market Model},
  journal = {Neural Networks},
  volume = {1},
  number = {4},
  pages = {339-356},
  year = {1988},
  issn = {0893-6080},
  doi = {10.1016/0893-6080(88)90007-X},
  url = {https://www.sciencedirect.com/science/article/pii/089360808890007X},
  author = {Werbos, Paul J.},
  series = {Neural Networks},
  keywords = {neural network type:recurrent, evaluation type:mathematical proof, evaluation type:empirical assessment},
  abstract = {Backpropagation is often viewed as a method for adapting artificial neural networks to classify patterns. Based on parts of the book by Rumelhart and colleagues, many authors equate backpropagation with the generalized delta rule applied to fully-connected feedforward networks. This paper will summarize a more general formulation of backpropagation, developed in 1974, which does more justice to the roots of the method in numerical analysis and statistics, and also does more justice to creative approaches expressed by neural modelers in the past year or two. It will discuss applications of backpropagation to forecasting over time (where errors have been halved by using methods other than least squares), to optimization, to sensitivity analysis, and to brain research. This paper will go on to derive a generalization of backpropagation to recurrent systems (which input their own output), such as hybrids of perceptron-style networks and Grossberg/Hopfield networks. Unlike the proposal of Rumelhart, Hinton, and Williams, this generalization does not require the storage of intermediate iterations to deal with continuous recurrence. This generalization was applied in 1981 to a model of natural gas markets, where it located sources of forecast uncertainty related to the use of least squares to estimate the model parameters in the first place.}
}
@article{HochreiterSchmidhuber1997,
  author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  title = {Long Short-Term Memory},
  journal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735-1780},
  year = {1997},
  month = {11},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  url = {https://doi.org/10.1162/neco.1997.9.8.1735},
  series = {Neural Computation},
  keywords = {neural network type:recurrent, evaluation type:mathematical proof, evaluation type:cost-benefit analysis},
  eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf}
}
@misc{LiptonBerkowitzElkan2015,
  title={A Critical Review of Recurrent Neural Networks for Sequence Learning}, 
  author = {Lipton, Zachary C. and Berkowitz, John and Elkan, Charles},
  year={2015},
  eprint={1506.00019},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  doi = {10.48550/arXiv.1506.00019},
  series = {arXiv},
  keywords = {neural network type:recurrent, evaluation type:survey or literature review},
  howpublished={unpublished},
  abstract = {Countless learning tasks require dealing with sequential data. Image captioning, speech synthesis, and music generation all require that a model produce outputs that are sequences. In other domains, such as time series prediction, video analysis, and musical information retrieval, a model must learn from inputs that are sequences. Interactive tasks, such as translating natural language, engaging in dialogue, and controlling a robot, often demand both capabilities. Recurrent neural networks (RNNs) are connectionist models that capture the dynamics of sequences via cycles in the network of nodes. Unlike standard feedforward neural networks, recurrent networks retain a state that can represent information from an arbitrarily long context window. Although recurrent neural networks have traditionally been difficult to train, and often contain millions of parameters, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful large-scale learning with them. In recent years, systems based on long short-term memory (LSTM) and bidirectional (BRNN) architectures have demonstrated ground-breaking performance on tasks as varied as image captioning, language translation, and handwriting recognition. In this survey, we review and synthesize the research that over the past three decades first yielded and then made practical these powerful learning models. When appropriate, we reconcile conflicting notation and nomenclature. Our goal is to provide a selfcontained explication of the state of the art together with a historical perspective and references to primary research.}
}
@article{KrizhevskySutskeverHinton2017,
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  title = {Image{N}et Classification with Deep Convolutional Neural Networks},
  year = {2017},
  issue_date = {June 2017},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {60},
  number = {6},
  issn = {0001-0782},
  url = {https://doi.org/10.1145/3065386},
  doi = {10.1145/3065386},
  abstract = {We trained a large, deep convolutional to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5 percent and 17.0 percent, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3 percent, compared to 26.2 percent achieved by the second-best entry.},
  journal = {Commun. ACM},
  month = {may},
  pages = {84-90},
  keywords = {neural network type:convolutional, evaluation type:comparison with previous method(s)},
  series = {Commun. ACM},
  numpages = {7}
}
@inproceedings{ZeilerFergus2014,
  author={Zeiler, Matthew D. and Fergus, Rob},
  editor={Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  title={Visualizing and Understanding Convolutional Networks},
  booktitle={Computer Vision -- ECCV 2014},
  year={2014},
  publisher={Springer International Publishing},
  address={Cham},
  pages={818--833},
  doi={10.1007/978-3-319-10590-1_53},
  series={ECCV},
  abstract={Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
  keywords = {neural network type:convolutional, evaluation type:comparison with previous method(s), evaluation type:empirical assessment},
  isbn={978-3-319-10590-1}
}
@misc{SimonyanZisserman2015,
  title={Very Deep Convolutional Networks for Large-Scale Image Recognition}, 
  author={Simonyan, Karen and Zisserman, Andrew},
  year={2015},
  eprint={1409.1556},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  series={ICLR},
  doi={10.48550/arXiv.1409.1556},
  keywords = {neural network type:convolutional, evaluation type:comparison with previous method(s)},
  howpublished={inproceedings},
  abstract={In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 x 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.}
}
@inproceedings{HeZhangRenSun2016,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
  volume={},
  number={},
  pages={770-778},
  series={CVPR},
  publisher={IEEE},
  keywords = {neural network type:convolutional, evaluation type:comparison with previous method(s), evaluation type:empirical assessment},
  doi={10.1109/CVPR.2016.90},
  abstract={Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8x deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57 percent error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28 percent relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.}
}
@inproceedings{AloysiusGeetha2017,
  author={Aloysius, Neena and Geetha, M.},
  booktitle={2017 International Conference on Communication and Signal Processing (ICCSP)},
  publisher={IEEE},
  title={A Review on Deep Convolutional Neural Networks}, 
  year={2017},
  volume={},
  number={},
  pages={0588-0592},
  doi={10.1109/ICCSP.2017.8286426},
  keywords = {neural network type:convolutional, evaluation type:survey or literature review},
  series={ICCSP},
  abstract={The success of traditional methods for solving computer vision problems heavily depends on the feature extraction process. But Convolutional Neural Networks (CNN) have provided an alternative for automatically learning the domain specific features. Now every problem in the broader domain of computer vision is re-examined from the perspective of this new methodology. Therefore it is essential to figure-out the type of network specific to a problem. In this work, we have done a thorough literature survey of Convolutional Neural Networks which is the widely used framework of deep learning. With AlexNet as the base CNN model, we have reviewed all the variations emerged over time to suit various applications and a small discussion on the available frameworks for the implementation of the same. We hope this piece of article will really serve as a guide for any neophyte in the area.}
}
